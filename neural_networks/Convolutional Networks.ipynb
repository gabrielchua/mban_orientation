{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we are going to introduce convolutional neural networks (CNNs) and experiment with some of their key hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, GlobalMaxPooling2D, MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications import VGG16\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We're going to get the data we will be working with throughout this module, MNIST, and grab the training and testing set. Additionally, we are going to down-sample the training set because the computation time might be a bit much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and testing data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Down-sample the training data\n",
    "rng = np.random.RandomState(17)\n",
    "idx = rng.choice(np.arange(X_train.shape[0]), size=10000, replace=False)\n",
    "X_train = X_train[idx, :]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row-Major vs Column-Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native NumPy, row-major format\n",
    "x = np.array([1, 2, 3, 4])\n",
    "x.reshape((2, 2), order=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-major format\n",
    "x.reshape((2, 2), order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of functions knowing that Python is row-major\n",
    "def copy_col(x):\n",
    "    n = len(x)\n",
    "    arr = np.empty(shape=(n, n))\n",
    "    for i in range(n):\n",
    "        arr[:, i] = x\n",
    "    \n",
    "    return None\n",
    "\n",
    "def copy_row(x):\n",
    "    n = len(x)\n",
    "    arr = np.empty(shape=(n, n))\n",
    "    for i in range(n):\n",
    "        arr[i, :] = x\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test these functions and compare which is quicker -- this can be easily done using\n",
    "# %%timeit kernel magic\n",
    "x = rng.randn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit copy_col(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit copy_row(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Before we begin training the convolutional network, we need to normalize our data\n",
    "\n",
    "Since we're working in Python, the data is already stored in row-major format, therefore the first exercise will not make sense. However, if you would like, you can practice converting to column-major format using the techniques shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Normalize the data using standard techniques\n",
    "\n",
    "**HINT**: It is valid with images to simply use the global mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the last channel to the training and testing data\n",
    "train_samples, height, width = X_train.shape\n",
    "X_train = X_train.reshape((train_samples, height, width, 1))\n",
    "\n",
    "test_samples = X_test.shape[0]\n",
    "X_test = X_test.reshape((test_samples, height, width, 1))\n",
    "\n",
    "# We can compute the mean and standard deviation using\n",
    "# functions from NumPY\n",
    "mu = X_train.mean()\n",
    "sigma = X_train.std()\n",
    "\n",
    "# Apply the normalization for each element of the matrices\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we need to one-hot encode our training vectors. This be done by typing\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "\n",
    "For this portion of the lecture, we will now practice working with CNNs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "For this exercise, I want you to use your knowledge of the Keras API to train a convolutional network with the following properties. You will need the layers: Conv2D and GlobalMaxPooling2D and I want the network to have the following specifications\n",
    "\n",
    "- One convolutional layer with 32 filters, (3, 3) kernel size and padding = \"same\"\n",
    "- One global pooling layer with default settings\n",
    "- One dense layer with 64 nodes\n",
    "- Standard SGD optimizer\n",
    "- ReLU activation function for each layer\n",
    "- Run the model for 3 epochs with a 25% validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, height, width, channels = X_train.shape\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", \n",
    "           padding=\"same\", input_shape=(height, width, channels)),\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=SGD(), loss=\"categorical_crossentropy\")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Using the kernel size that was assigned to your group, implement a neural network with the same parameters as before except with the specified kernel size. Make sure that you are typing out the full model and not just copying-and-pasting what you have done previously; report the performance on the test set and compare this to the previous model we trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The value of this exercise is coding this out yourself. If you would like, copy-paste the code I provided above and try different values for the hyper-parameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "For the final exercise of this section, I want you to add one more convolutional layer (w/ 32 filters) and a default max_pooling layer between it Use a kernel size of 7 and keep everything else the same; remember to type out the full model so that you're getting used to the API and repor the model's final performance on the test set; compare this to how we did with a single layer convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will provide the model -- you can copy the compile and fit steps since they don't change\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(7, 7), activation=\"relu\", \n",
    "           padding=\"same\", input_shape=(height, width, channels)),\n",
    "    MaxPooling2d(), # Added the pooling layer between the convolutional ones\n",
    "    Conv2D(filters=32, kernel_size=(7, 7), activation=\"relu\"), # Added the second convolutional layer\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "This is not an exercise, I just want to make you aware of how you can employ this technique because it's a common practice used for real-world problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the VGG16 weights\n",
    "vgg = VGG16(include_top=False, input_shape=(128, 128, 3), pooling=\"max\")\n",
    "\n",
    "# Free the weights in the VGG model\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Create a new model using the VGG16 weights\n",
    "model = Sequential([\n",
    "    vgg,\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the new model\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

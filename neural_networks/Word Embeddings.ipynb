{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "For this final section we're going to work on word embeddings and how we can use them to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Again we're going to get the training and testing data that have been provided to us and then perform a bit of exploratory analysis help do some additional pre-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw training and testing data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is in a somewhat unusual format; let's take a look at how it's \n",
    "# currently formated and how we'll need to adjust it to be used by Keras\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Typically, natural language processing problems are very skewed -- namely, a small number of words cover most of the uses in the data. If this is true, then this typically implies we can shrink the vocabulary without paying a big price in terms of model performance while significantly speeding up computation time (similar to PCA). To check this hypothesis, our first exercise for this project will be to look at the distribution of words in the data. Specifically, I would like you to create a histogram displaying the word usage distribution in each of the reviews. To do this, you will need to represent the training data as a DataFrame and use this DataFrame to make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the create_word_df function\n",
    "\n",
    "# An example of a valid output for this function would look like\n",
    "\n",
    "# sample_num_vect | words\n",
    "# -----------------------\n",
    "# 1               | 15\n",
    "# 1               | 27\n",
    "# 1               | 3\n",
    "# ...\n",
    "\n",
    "def create_word_df(word_vect, sample_num):\n",
    "    # Repeat the sample_num an appropriate number of times\n",
    "    sample_num_vect = np.repeat(sample_num, len(word_vect))\n",
    "    \n",
    "    # Return a DataFrame with two columns: the sample_num_vect and words\n",
    "    return pd.DataFrame({\"sample_num_vect\": sample_num_vect,\n",
    "                         \"words\": word_vect})\n",
    "    \n",
    "# Apply create_word_df to each element of the X_train data\n",
    "word_df = pd.concat(list(map(create_word_df, X_train, range(len(X_train)))),\n",
    "                    ignore_index=True)\n",
    "\n",
    "# Using the word_df DataFrame, plot the distribution of words in the data\n",
    "word_df[\"words\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Before we can use a word embedding model, each of the word vectors need to have the same size; therefore, we need to see the distribution of review lengths in the data. Please generate a boxplot displaying the distribution of review lengths\n",
    "\n",
    "**HINT:** \"count\" is an aggregation function that can be used to tell you the number of rows in a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to group by the sample_num_vect,\n",
    "# count the number of rows per entry, and then \n",
    "# plot this using Pandas\n",
    "word_df.groupby(\"sample_num_vect\").agg(\"count\").boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have determined the appropriate vocabulary and vector lengths for our dataset we can now get these values from the IMDB data. Fortunately since this is such a commonly used dataset, these functions are included for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and testing data with the new constraints\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000, maxlen=500, seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we need to pad the sequences to ensure that all of the vectors have the same\n",
    "# length\n",
    "X_train = pad_sequences(X_train, maxlen=500)\n",
    "X_test = pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Now we are going to introduce word embeddings in the context of sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "For this exercise, using the Keras API and the following layers: \n",
    "- Embedding\n",
    "- GlobalAveragePooling1D\n",
    "\n",
    "Generate a neural network with the following hyper-parameters\n",
    "\n",
    "- Embedding layer with 32 dimensional word vectors\n",
    "- Default GlobalAveragePooling1D\n",
    "- One dense layer with 64 units\n",
    "- Standard SGD optimizer\n",
    "- Binary cross-entropy loss function\n",
    "- Train for five epochs\n",
    "\n",
    "When this is done, evaluate the model out-of-sample\n",
    "\n",
    "**HINT**: when you have a binary output, the final activation function is \"sigmoid\" and only has one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trick is to know which arguments to pass to Embedding()\n",
    "# input_dim => # of vocab words (5000)\n",
    "# output_dim => # number of dim to represent words (32)\n",
    "# input_length => # expected length of vectors (500)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=32, input_length=500),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Optimize the model the same way we've done before\n",
    "model.compile(optimizer=SGD(), loss=\"binary_crossentropy\")\n",
    "\n",
    "# Fit the model to data\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Using the optimization algorithm that was assigned to your group, define and train an embedding model that has the same specifications as before; plot the history and report the final test error. Remember to type out the neural network and do not just copy-paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll show the example for the Adam() optimizer; it's very similar for \n",
    "# the other ones\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=32, input_length=500),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# We just have to change from SGD() to Adam() -- that's how easy it is!\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\")\n",
    "\n",
    "# Fit the model to data\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "In our previous model, we just arbitrarily chose the words to be represented\n",
    "by 32-dimensional vectors; let's see how sensitive our model is to that\n",
    "choice; using either a 4, 128, or 256 dimensional vector with the Adam \n",
    "optimizer, determine how sensitive the model is to this hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll show you how to do this for 128; just play around with the \n",
    "# `output_dim` hyper-parameter; we'll still use the Adam() optimizer\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=500), # just change the output_dim\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# We just have to change from SGD() to Adam() -- that's how easy it is!\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\")\n",
    "\n",
    "# Fit the model to data\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
